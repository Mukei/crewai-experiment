# LLM Configuration for Ollama
ollama_llm:
  model: "ollama/deepseek-r1"  # Using DeepSeek-R1 model as originally planned
  base_url: "http://localhost:11434"  # Only use base_url, not api_base
  temperature: 0.7
  api_key: "not-needed"  # Required by LiteLLM but not used by Ollama
  max_tokens: 2048  # Specify max tokens